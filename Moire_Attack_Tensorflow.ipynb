{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c88de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from mosaicing_demosaicing_v2 import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.utils\n",
    "from torchvision import models\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torchattacks\n",
    "from torchattacks.attack import Attack\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Resize\n",
    "\n",
    "from image_transformer import ImageTransformer\n",
    "from utils import *\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3d20dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morie_attack(Attack):\n",
    "    r\"\"\"\n",
    "    Distance Measure : L_inf bound on sensor noise\n",
    "    Arguments:\n",
    "        model (nn.Module): Victim model to attack.\n",
    "        steps (int): number of steps. (DEFAULT: 50)\n",
    "        batch_size (int): batch size\n",
    "        scale_factor (int): zoom in the images on the LCD. （DEFAULT: 3）\n",
    "\n",
    "    Shape:\n",
    "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`, `H = height` and `W = width`. It must have a range [0, 1].\n",
    "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
    "        - output: :math:`(N, C, H, W)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, img_h, img_w, noise_budget, scale_factor, steps = 50, batch_size = 50, targeted = False):\n",
    "        super(Morie_attack, self).__init__(\"Morie_attack\", model)\n",
    "        self.steps = steps\n",
    "        self.targeted = targeted\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.scale_factor = scale_factor\n",
    "        self.noise_budget = noise_budget\n",
    "        self.lr = noise_budget / steps\n",
    "        noise = np.zeros([batch_size, self.img_h * self.scale_factor * 3, self.img_w * self.scale_factor * 3])\n",
    "        self.noise = torch.from_numpy(noise).to(self.device)\n",
    "        self.noise.requires_grad = True\n",
    "        self.adv_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "    def simulate_LCD_display(self, input_img):\n",
    "        \"\"\" Simulate the display of raw images on LCD screen\n",
    "        Input:\n",
    "            original images (tensor): batch x height x width x channel\n",
    "        Output:\n",
    "            LCD images (tensor): batch x (height x scale_factor)  x (width x scale_factor) x channel\n",
    "        \"\"\"\n",
    "        input_img = np.asarray(input_img.cpu().detach())\n",
    "        batch_size, h, w, c = input_img.shape\n",
    "\n",
    "        simulate_imgs = np.zeros((batch_size, h * 3, w * 3, 3), dtype=np.float32)\n",
    "        red = np.repeat(input_img[:, :, :, 0], 3, axis = 1)\n",
    "        green = np.repeat(input_img[:, :, :, 1], 3, axis = 1)\n",
    "        blue = np.repeat(input_img[:, :, :, 2], 3, axis = 1)\n",
    "\n",
    "        for y in range(w):\n",
    "            simulate_imgs[:, :, y * 3, 0] = red[:, :, y]\n",
    "            simulate_imgs[:, :, y * 3 + 1, 1] = green[:, :, y]\n",
    "            simulate_imgs[:, :, y * 3 + 2, 2] = blue[:, :, y]\n",
    "        simulate_imgs = torch.from_numpy(simulate_imgs).to(self.device)\n",
    "\n",
    "        return simulate_imgs\n",
    "\n",
    "    def demosaic_and_denoise(self, input_img):\n",
    "        \"\"\" Apply demosaicing to the images\n",
    "        Input:\n",
    "            images (tensor): batch x (height x scale_factor) x (width x scale_factor)\n",
    "        Output:\n",
    "            demosaicing images (tensor): batch x (height x scale_factor) x (width x scale_factor) x channel\n",
    "        \"\"\"\n",
    "        demosaicing_imgs = demosaicing_CFA_Bayer_bilinear(input_img)\n",
    "        return demosaicing_imgs\n",
    "\n",
    "    def simulate_CFA(self, input_img):\n",
    "        \"\"\" Simulate the raw reading of the camera sensor using bayer CFA\n",
    "        Input:\n",
    "            images (tensor): batch x (height x scale_factor) x (width x scale_factor) x channel\n",
    "        Output:\n",
    "            mosaicing images (tensor): batch x (height x scale_factor) x (width x scale_factor)\n",
    "        \"\"\"\n",
    "        mosaicing_imgs = mosaicing_CFA_Bayer(input_img)\n",
    "        return mosaicing_imgs\n",
    "\n",
    "    def random_rotation_3(self, org_images, lcd_images):\n",
    "        \"\"\" Simulate the 3D rotatation during the shooting\n",
    "        Input:\n",
    "            images (tensor): batch x height x width x channel\n",
    "        Rotate angle:\n",
    "            theta (int): (-20, 20)\n",
    "            phi (int): (-20, 20)\n",
    "            gamma (int): (-20, 20)\n",
    "        Output:\n",
    "            rotated original images (tensor): batch x height x width x channel\n",
    "            rotated LCD images (tensor): batch x (height x scale_factor) x (width x scale_factor) x channel\n",
    "        \"\"\"\n",
    "        rotate_images = np.zeros(org_images.size())\n",
    "        rotate_lcd_images = np.zeros(lcd_images.size())\n",
    "\n",
    "        for n, img in enumerate(org_images):\n",
    "            Trans_org = ImageTransformer(img)\n",
    "            theta, phi, gamma, rotate_img = Trans_org.rotate_along_axis(True)\n",
    "            rotate_images[n, :] = rotate_img\n",
    "            Trans_lcd = ImageTransformer(lcd_images[n])\n",
    "            _, _, _, rotate_lcd_img = Trans_lcd.rotate_along_axis(False, theta, phi, gamma)\n",
    "            rotate_lcd_images[n, :] = rotate_lcd_img\n",
    "\n",
    "        rotate_images = torch.from_numpy(rotate_images).to(device)\n",
    "        rotate_lcd_images = torch.from_numpy(rotate_lcd_images).to(device)\n",
    "\n",
    "        return rotate_images, rotate_lcd_images\n",
    "\n",
    "    def forward(self, org_imgs, org_labels, targeted_labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        org_images = org_imgs.clone().detach().to(self.device)\n",
    "        org_labels = org_labels.clone().detach().to(self.device)\n",
    "        org_labels = self._transform_label(org_images, org_labels)\n",
    "\n",
    "        # compute the orignal prediction\n",
    "        temp_outputs = self.model(org_imgs.clone().detach().to(self.device))\n",
    "        org_percentage = F.softmax(temp_outputs, dim=1) * 100\n",
    "        del temp_outputs\n",
    "\n",
    "        resize_before_lcd = F.interpolate(org_images, scale_factor = self.scale_factor, mode=\"bilinear\")\n",
    "        resize_before_lcd = resize_before_lcd.permute(0, 2, 3, 1)\n",
    "        lcd_images = self.simulate_LCD_display(resize_before_lcd)\n",
    "\n",
    "        temp_images = org_images.clone().detach().permute(0, 2, 3, 1)\n",
    "\n",
    "        rotate_images, rotate_lcd_images = self.random_rotation_3(temp_images, lcd_images)\n",
    "        rotate_images = rotate_images.to(self.device)\n",
    "        rotate_lcd_images = rotate_lcd_images.to(self.device).detach()\n",
    "\n",
    "        dim_images = adjust_contrast_and_brightness(rotate_images, beta=-60)\n",
    "\n",
    "        ## compute the rotate prediction\n",
    "        rotate_images = rotate_images.permute(0, 3, 1, 2)\n",
    "        rotate_images = rotate_images.float()\n",
    "        rotate_outputs = self.model(rotate_images)\n",
    "        _, rotate_pre = torch.max(rotate_outputs.data, 1)\n",
    "        rotate_percentage = F.softmax(rotate_outputs.clone().detach(), dim=1) * 100\n",
    "\n",
    "        ## compute the dim prediction\n",
    "        dim_images = dim_images.permute(0, 3, 1, 2)\n",
    "        dim_images = dim_images.float()\n",
    "        dim_outputs = self.model(dim_images)\n",
    "        _, dim_pre = torch.max(dim_outputs.data, 1)\n",
    "        dim_percentage = F.softmax(dim_outputs.clone().detach(), dim=1) * 100\n",
    "\n",
    "\n",
    "        ## Deliver the MA\n",
    "        for step in range(self.steps):\n",
    "            print(\"Step: {}/{}\".format(step, self.steps))\n",
    "\n",
    "            cfa_img = self.simulate_CFA(rotate_lcd_images)\n",
    "            cfa_img_noise = cfa_img + self.noise\n",
    "\n",
    "            demosaic_img = self.demosaic_and_denoise(cfa_img_noise)\n",
    "            demosaic_img = demosaic_img.permute(0, 3, 1, 2)\n",
    "\n",
    "            ## Adjust the brightness\n",
    "            brighter_img = adjust_contrast_and_brightness(demosaic_img, beta=20)\n",
    "\n",
    "            at_images = F.interpolate(brighter_img, [299, 299], mode='bilinear')\n",
    "            at_images = at_images.float()\n",
    "            at_outputs = self.model(at_images)\n",
    "            _, at_pre = torch.max(at_outputs.data, 1)\n",
    "\n",
    "            at_percentage = F.softmax(at_outputs.clone().detach(), dim=1) * 100\n",
    "\n",
    "\n",
    "            if self.targeted:\n",
    "                adv_cost = self.adv_loss(at_outputs, (targeted_labels.to(self.device)).long())\n",
    "            else:\n",
    "                adv_cost = -1 * self.adv_loss(at_outputs, org_labels)\n",
    "\n",
    "\n",
    "            total_cost = adv_cost\n",
    "            print(\"Loss: \", total_cost, \"Adv loss: \", adv_cost)\n",
    "\n",
    "            total_cost.backward()\n",
    "            gradient = self.noise.grad\n",
    "            self.noise = self.noise.detach() - self.lr * torch.sign(gradient)\n",
    "            self.noise = torch.clamp(self.noise, min = -self.noise_budget, max = self.noise_budget).detach()\n",
    "            self.noise.requires_grad = True\n",
    "\n",
    "        at_images = torch.clamp(at_images, min=0, max=255).detach()\n",
    "\n",
    "        return at_images, rotate_images, dim_images, \\\n",
    "               at_pre, rotate_pre, dim_pre, \\\n",
    "               org_percentage, at_percentage, rotate_percentage, dim_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99aaa968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.register_buffer('mean', torch.Tensor(mean))\n",
    "        self.register_buffer('std', torch.Tensor(std))\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input / 255.0\n",
    "        mean = self.mean.reshape(1, 3, 1, 1)\n",
    "        std = self.std.reshape(1, 3, 1, 1)\n",
    "        return (input - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3bfa578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\envs\\Moire_Attack_env\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "C:\\ProgramData\\miniconda3\\envs\\Moire_Attack_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------\n",
      "Noise_budget =  2\n",
      "----------------------------------------------------------------------\n",
      "Epoch = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\miniconda3\\envs\\Moire_Attack_env\\lib\\site-packages\\ipykernel_launcher.py:62: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0/10\n",
      "Loss:  tensor(9.1622, grad_fn=<NllLossBackward0>) Adv loss:  tensor(9.1622, grad_fn=<NllLossBackward0>)\n",
      "Step: 1/10\n",
      "Loss:  tensor(7.7545, grad_fn=<NllLossBackward0>) Adv loss:  tensor(7.7545, grad_fn=<NllLossBackward0>)\n",
      "Step: 2/10\n",
      "Loss:  tensor(6.8366, grad_fn=<NllLossBackward0>) Adv loss:  tensor(6.8366, grad_fn=<NllLossBackward0>)\n",
      "Step: 3/10\n",
      "Loss:  tensor(6.1821, grad_fn=<NllLossBackward0>) Adv loss:  tensor(6.1821, grad_fn=<NllLossBackward0>)\n",
      "Step: 4/10\n",
      "Loss:  tensor(5.7224, grad_fn=<NllLossBackward0>) Adv loss:  tensor(5.7224, grad_fn=<NllLossBackward0>)\n",
      "Step: 5/10\n",
      "Loss:  tensor(5.2752, grad_fn=<NllLossBackward0>) Adv loss:  tensor(5.2752, grad_fn=<NllLossBackward0>)\n",
      "Step: 6/10\n",
      "Loss:  tensor(4.6812, grad_fn=<NllLossBackward0>) Adv loss:  tensor(4.6812, grad_fn=<NllLossBackward0>)\n",
      "Step: 7/10\n",
      "Loss:  tensor(4.1665, grad_fn=<NllLossBackward0>) Adv loss:  tensor(4.1665, grad_fn=<NllLossBackward0>)\n",
      "Step: 8/10\n",
      "Loss:  tensor(3.8677, grad_fn=<NllLossBackward0>) Adv loss:  tensor(3.8677, grad_fn=<NllLossBackward0>)\n",
      "Step: 9/10\n",
      "Loss:  tensor(3.3925, grad_fn=<NllLossBackward0>) Adv loss:  tensor(3.3925, grad_fn=<NllLossBackward0>)\n",
      "Current rotate Suc rate:  tensor(0.1000)\n",
      "Current dim Suc rate:  tensor(0.1000)\n",
      "Current attack Suc rate:  tensor(0.3000)\n",
      "Total rotate Suc rate:  tensor(0.1000)\n",
      "Total dim Suc rate:  tensor(0.1000)\n",
      "Total attack Suc rate:  tensor(0.3000)\n",
      "Rotate Success rate:  tensor(0.1000)\n",
      "dim Success rate:  tensor(0.1000)\n",
      "Attack Success rate:  tensor(0.3000)\n"
     ]
    }
   ],
   "source": [
    "STEPS = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_idx = json.load(open(\"./data/imagenet_class_index.json\"))\n",
    "idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "class2label = [class_idx[str(k)][0] for k in range(len(class_idx))]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.ToTensor(), ])\n",
    "\n",
    "norm_layer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "model = nn.Sequential(\n",
    "    norm_layer,\n",
    "    models.inception_v3(pretrained=True)\n",
    ").to(device)\n",
    "\n",
    "model = model.eval()\n",
    "\n",
    "## Save the results of MA\n",
    "## SET TO TRUE IF WE WANT TO SAVE THE OUTPUT IMAGES\n",
    "Save_results = 'True'\n",
    "if Save_results == 'True':\n",
    "    savedir = './Results'\n",
    "    adv_dir = os.path.join(savedir, 'adv')\n",
    "    rotate_dir = os.path.join(savedir, 'rotate')\n",
    "    org_dir = os.path.join(savedir, 'org')\n",
    "    dim_dir = os.path.join(savedir, 'dim')\n",
    "    create_dir(adv_dir)\n",
    "    create_dir(rotate_dir)\n",
    "    create_dir(org_dir)\n",
    "    create_dir(dim_dir)\n",
    "\n",
    "## deffault settings\n",
    "noise_budget = 2\n",
    "batch_size = 10\n",
    "epoch = 1#int(1000 / batch_size)\n",
    "total = 0\n",
    "suc_cnt_at = 0\n",
    "suc_cnt_dim = 0\n",
    "suc_cnt_rotate = 0\n",
    "\n",
    "\n",
    "\n",
    "normal_data = image_folder_custom_label(root='./data/dataset/incepv3_data', transform=transform,\n",
    "                                        idx2label=class2label)\n",
    "normal_loader = torch.utils.data.DataLoader(normal_data, batch_size=batch_size, shuffle=False)\n",
    "normal_iter = iter(normal_loader)\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(\"Noise_budget = \", noise_budget)\n",
    "start = time.time()\n",
    "for batch in range(epoch):\n",
    "\n",
    "    print(\"-\" * 70)\n",
    "    org_imgs, org_labels = next(normal_iter)\n",
    "    org_imgs = org_imgs * 255.0\n",
    "    print('Epoch = ' + str(batch))\n",
    "\n",
    "    targeted_labels = np.random.randint(0,999)\n",
    "    targeted_labels = torch.from_numpy(targeted_labels * np.ones((batch_size), dtype = np.int))\n",
    "    targeted = True\n",
    "\n",
    "\n",
    "    attack = Morie_attack(model,\n",
    "                          noise_budget=noise_budget,\n",
    "                          img_w=299,\n",
    "                          img_h=299,\n",
    "                          scale_factor=3,\n",
    "                          targeted=targeted,\n",
    "                          batch_size=batch_size,\n",
    "                          steps=STEPS)\n",
    "    \n",
    "    at_images, rotate_images, dim_images, \\\n",
    "    at_labels, rotate_labels, dim_labels, \\\n",
    "    org_percentges, at_percentages, rotate_percentages, dim_percentages = attack(org_imgs, org_labels, targeted_labels)\n",
    "    org_labels = org_labels.to(device)\n",
    "    targeted_labels = targeted_labels.to(device)\n",
    "    rotate_labels = rotate_labels.to(device)\n",
    "\n",
    "    ## compute the succes rate\n",
    "    total += batch_size\n",
    "\n",
    "    suc_cnt_rotate += (rotate_labels != org_labels).sum()\n",
    "    suc_cnt_dim += (dim_labels != org_labels).sum()\n",
    "\n",
    "    if targeted:\n",
    "        suc_cnt_at += (at_labels == targeted_labels).sum()\n",
    "    else:\n",
    "        suc_cnt_at += (at_labels != org_labels).sum()\n",
    "\n",
    "\n",
    "    Succ_cnt_rotate = (rotate_labels != org_labels).sum() / batch_size\n",
    "    Succ_total_rotate = suc_cnt_rotate / total\n",
    "    Succ_cnt_dim = (dim_labels != org_labels).sum() / batch_size\n",
    "    Succ_total_dim = suc_cnt_dim / total\n",
    "    if targeted:\n",
    "        Succ_cnt_at = (at_labels == targeted_labels).sum() / batch_size\n",
    "        Succ_total_at = suc_cnt_at / total\n",
    "    else:\n",
    "        Succ_cnt_at = (at_labels != org_labels).sum() / batch_size\n",
    "        Succ_total_at = suc_cnt_at / total\n",
    "\n",
    "    print(\"Current rotate Suc rate: \", Succ_cnt_rotate)\n",
    "    print(\"Current dim Suc rate: \", Succ_cnt_dim)\n",
    "    print(\"Current attack Suc rate: \", Succ_cnt_at)\n",
    "    print(\"Total rotate Suc rate: \", Succ_total_rotate)\n",
    "    print(\"Total dim Suc rate: \", Succ_total_dim)\n",
    "    print(\"Total attack Suc rate: \", Succ_total_at)\n",
    "\n",
    "    labels_np = org_labels.cpu().detach().numpy()\n",
    "    rotate_labels_np = rotate_labels.cpu().detach().numpy()\n",
    "    dim_labels_np = dim_labels.cpu().detach().numpy()\n",
    "    at_labels_np = at_labels.cpu().detach().numpy()\n",
    "\n",
    "    org_images_np = org_imgs.detach().cpu().numpy()\n",
    "    at_images_np = at_images.detach().cpu().numpy()\n",
    "    rotate_images_np = rotate_images.detach().cpu().numpy()\n",
    "    dim_images_np = dim_images.detach().cpu().numpy()\n",
    "\n",
    "    org_percentages_np = org_percentges.detach().cpu().numpy()\n",
    "    at_percentages_np = at_percentages.detach().cpu().numpy()\n",
    "    rotate_percentages_np = rotate_percentages.detach().cpu().numpy()\n",
    "    dim_percentages_np = dim_percentages.detach().cpu().numpy()\n",
    "\n",
    "    # save the pics\n",
    "    for i in range(batch_size):\n",
    "        img_org = org_images_np[i]\n",
    "        img_at = at_images_np[i]\n",
    "        img_dim = dim_images_np[i]\n",
    "        img_rotate = rotate_images_np[i]\n",
    "\n",
    "        img_org = np.moveaxis(img_org, 0, 2)\n",
    "        img_at = np.moveaxis(img_at, 0, 2)\n",
    "        img_dim = np.moveaxis(img_dim, 0, 2)\n",
    "        img_rotate = np.moveaxis(img_rotate, 0, 2)\n",
    "\n",
    "        true_class = idx2label[labels_np[i]]\n",
    "        at_class = idx2label[at_labels_np[i]]\n",
    "        dim_class = idx2label[dim_labels_np[i]]\n",
    "        rotate_class = idx2label[rotate_labels_np[i]]\n",
    "\n",
    "        percentage_org = org_percentages_np[i][labels_np[i]]\n",
    "        percentage_at = at_percentages_np[i][at_labels_np[i]]\n",
    "        percentage_rotate = rotate_percentages_np[i][rotate_labels_np[i]]\n",
    "        percentage_dim = dim_percentages_np[i][dim_labels_np[i]]\n",
    "\n",
    "        if Save_results == 'True':\n",
    "            # save org_images\n",
    "            img_org_name = true_class + str(percentage_org) + \".JPEG\"\n",
    "            img_org_path = os.path.join(org_dir, img_org_name)\n",
    "            img_org_pil = Image.fromarray(img_org.astype(np.uint8))\n",
    "            img_org_pil.save(img_org_path)\n",
    "\n",
    "            # uncomment the following if you want to save the intermediate results\n",
    "            # save rotated_images:\n",
    "            img_rotate_name = true_class + str(percentage_org) + \"_\" + rotate_class + str(percentage_rotate) + \".JPEG\"\n",
    "            img_rotate_path = os.path.join(rotate_dir, img_rotate_name)\n",
    "            img_rotate_pil = Image.fromarray(img_rotate.astype(np.uint8))\n",
    "            img_rotate_pil.save(img_rotate_path)\n",
    "\n",
    "            # save dim_images:\n",
    "            img_dim_name = true_class + str(percentage_org) + \"_\" + dim_class + str(percentage_dim) + \".JPEG\"\n",
    "            img_dim_path = os.path.join(dim_dir, img_dim_name)\n",
    "            img_dim_pil = Image.fromarray(img_dim.astype(np.uint8))\n",
    "            img_dim_pil.save(img_dim_path)\n",
    "\n",
    "            ##  save at_images:\n",
    "            img_at_name = true_class + str(percentage_org) + \"_\" + at_class + str(percentage_at) + \".JPEG\"\n",
    "            img_at_path = os.path.join(adv_dir, img_at_name)\n",
    "            img_at_pil = Image.fromarray(img_at.astype(np.uint8))\n",
    "            img_at_pil.save(img_at_path)\n",
    "\n",
    "\n",
    "\n",
    "    del attack, at_images, rotate_images, dim_images, \\\n",
    "        at_labels, rotate_labels, dim_labels, \\\n",
    "        org_percentges, at_percentages, rotate_percentages, dim_percentages, \\\n",
    "        org_labels, targeted_labels, \\\n",
    "        org_imgs\n",
    "\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Rotate Success rate: \", Succ_total_rotate)\n",
    "print(\"dim Success rate: \", Succ_total_dim)\n",
    "print(\"Attack Success rate: \", Succ_total_at)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Moire_Attack_env",
   "language": "python",
   "name": "moire_attack_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
