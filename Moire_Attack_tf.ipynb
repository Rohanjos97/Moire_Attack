{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c88de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohan\\AppData\\Roaming\\Python\\Python37\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#Not available direct APIs\\nimport torchvision.utils\\nimport torchattacks # Using cleverhans instead of this(if we can use it)\\nfrom torchattacks.attack import Attack\\nfrom torchvision.transforms import Compose, CenterCrop, ToTensor, Resize # use below code instead, if applicable\\n# Define the transformations\\ntransform = ImageDataGenerator(\\n    rotation_range=15,\\n    width_shift_range=0.1,\\n    height_shift_range=0.1,\\n    shear_range=0.2,\\n    zoom_range=0.2,\\n    horizontal_flip=True,\\n    fill_mode='nearest')\\n\\n# Load and preprocess an image\\nimg = tf.keras.preprocessing.image.load_img('image.jpg', target_size=(224, 224))\\nimg = tf.keras.preprocessing.image.img_to_array(img)\\nimg = img.reshape((1,) + img.shape)\\nimg = transform.flow(img).next()\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from mosaicing_demosaicing_v2_tf import *\n",
    "from image_transformer_tf import ImageTransformer\n",
    "from utils_tf import *\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import nn\n",
    "from tensorflow.keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "'''\n",
    "#Not available direct APIs\n",
    "import torchvision.utils\n",
    "import torchattacks # Using cleverhans instead of this(if we can use it)\n",
    "from torchattacks.attack import Attack\n",
    "from torchvision.transforms import Compose, CenterCrop, ToTensor, Resize # use below code instead, if applicable\n",
    "# Define the transformations\n",
    "transform = ImageDataGenerator(\n",
    "    rotation_range=15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "# Load and preprocess an image\n",
    "img = tf.keras.preprocessing.image.load_img('image.jpg', target_size=(224, 224))\n",
    "img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img = img.reshape((1,) + img.shape)\n",
    "img = transform.flow(img).next()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3d20dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Morie_attack():\n",
    "    r\"\"\"\n",
    "    Distance Measure : L_inf bound on sensor noise\n",
    "    Arguments:\n",
    "        model (nn.Module): Victim model to attack.\n",
    "        steps (int): number of steps. (DEFAULT: 50)\n",
    "        batch_size (int): batch size\n",
    "        scale_factor (int): zoom in the images on the LCD. （DEFAULT: 3）\n",
    "\n",
    "    Shape:\n",
    "        - images: :math:`(N, C, H, W)` where `N = number of batches`, `C = number of channels`, `H = height` and `W = width`. It must have a range [0, 1].\n",
    "        - labels: :math:`(N)` where each value :math:`y_i` is :math:`0 \\leq y_i \\leq` `number of labels`.\n",
    "        - output: :math:`(N, C, H, W)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, img_h, img_w, noise_budget, scale_factor, steps = 50, batch_size = 50, targeted = False):\n",
    "        super(Morie_attack, self).__init__(\"Morie_attack\", model)\n",
    "        self.steps = steps\n",
    "        self.targeted = targeted\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.scale_factor = scale_factor\n",
    "        self.noise_budget = noise_budget\n",
    "        self.lr = noise_budget / steps\n",
    "        noise = np.zeros([batch_size, self.img_h * self.scale_factor * 3, self.img_w * self.scale_factor * 3])\n",
    "        self.noise = tf.convert_to_tensor(noise)\n",
    "        self.noise.requires_grad = True\n",
    "        self.adv_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def simulate_LCD_display(self, input_img):\n",
    "        \"\"\" Simulate the display of raw images on LCD screen\n",
    "        Input:\n",
    "            original images (tensor): batch x height x width x channel\n",
    "        Output:\n",
    "            LCD images (tensor): batch x (height x scale_factor)  x (width x scale_factor) x channel\n",
    "        \"\"\"\n",
    "        input_img = input_img.numpy()\n",
    "        batch_size, h, w, c = input_img.shape\n",
    "\n",
    "        simulate_imgs = np.zeros((batch_size, h * 3, w * 3, 3), dtype=np.float32)\n",
    "        red = np.repeat(input_img[:, :, :, 0], 3, axis = 1)\n",
    "        green = np.repeat(input_img[:, :, :, 1], 3, axis = 1)\n",
    "        blue = np.repeat(input_img[:, :, :, 2], 3, axis = 1)\n",
    "\n",
    "        for y in range(w):\n",
    "            simulate_imgs[:, :, y * 3, 0] = red[:, :, y]\n",
    "            simulate_imgs[:, :, y * 3 + 1, 1] = green[:, :, y]\n",
    "            simulate_imgs[:, :, y * 3 + 2, 2] = blue[:, :, y]\n",
    "        simulate_imgs = tf.convert_to_tensor(simulate_imgs)\n",
    "\n",
    "        return simulate_imgs\n",
    "\n",
    "    def demosaic_and_denoise(self, input_img):\n",
    "        \"\"\" Apply demosaicing to the images\n",
    "        Input:\n",
    "            images (tensor): batch x (height x scale_factor) x (width x scale_factor)\n",
    "        Output:\n",
    "            demosaicing images (tensor): batch x (height x scale_factor) x (width x scale_factor) x channel\n",
    "        \"\"\"\n",
    "        demosaicing_imgs = demosaicing_CFA_Bayer_bilinear(input_img)\n",
    "        return demosaicing_imgs\n",
    "\n",
    "    def simulate_CFA(self, input_img):\n",
    "        \"\"\" Simulate the raw reading of the camera sensor using bayer CFA\n",
    "        Input:\n",
    "            images (tensor): batch x (height x scale_factor) x (width x scale_factor) x channel\n",
    "        Output:\n",
    "            mosaicing images (tensor): batch x (height x scale_factor) x (width x scale_factor)\n",
    "        \"\"\"\n",
    "        mosaicing_imgs = mosaicing_CFA_Bayer(input_img)\n",
    "        return mosaicing_imgs\n",
    "\n",
    "    def random_rotation_3(self, org_images, lcd_images):\n",
    "        \"\"\" Simulate the 3D rotatation during the shooting\n",
    "        Input:\n",
    "            images (tensor): batch x height x width x channel\n",
    "        Rotate angle:\n",
    "            theta (int): (-20, 20)\n",
    "            phi (int): (-20, 20)\n",
    "            gamma (int): (-20, 20)\n",
    "        Output:\n",
    "            rotated original images (tensor): batch x height x width x channel\n",
    "            rotated LCD images (tensor): batch x (height x scale_factor) x (width x scale_factor) x channel\n",
    "        \"\"\"\n",
    "        rotate_images = np.zeros(org_images.size())\n",
    "        rotate_lcd_images = np.zeros(lcd_images.size())\n",
    "\n",
    "        for n, img in enumerate(org_images):\n",
    "            Trans_org = ImageTransformer(img)\n",
    "            theta, phi, gamma, rotate_img = Trans_org.rotate_along_axis(True)\n",
    "            rotate_images[n, :] = rotate_img\n",
    "            Trans_lcd = ImageTransformer(lcd_images[n])\n",
    "            _, _, _, rotate_lcd_img = Trans_lcd.rotate_along_axis(False, theta, phi, gamma)\n",
    "            rotate_lcd_images[n, :] = rotate_lcd_img\n",
    "\n",
    "        rotate_images = tf.convert_to_tensor(rotate_images)\n",
    "        rotate_lcd_images = tf.convert_to_tensor(rotate_lcd_images)\n",
    "\n",
    "        return rotate_images, rotate_lcd_images\n",
    "\n",
    "    def forward(self, org_imgs, org_labels, targeted_labels):\n",
    "        r\"\"\"\n",
    "        Overridden.\n",
    "        \"\"\"\n",
    "        org_images = org_imgs.clone()\n",
    "        org_labels = org_labels.clone()\n",
    "        org_labels = self._transform_label(org_images, org_labels)\n",
    "\n",
    "        # compute the orignal prediction\n",
    "        temp_outputs = self.model(org_imgs.clone())\n",
    "        org_percentage = tf.nn.softmax(temp_outputs, dim=1) * 100 #change dim to axis\n",
    "        del temp_outputs\n",
    "        \n",
    "        resize_before_lcd = tf.image.resize(org_images, scale_factor = self.scale_factor, method=\"bilinear\") #check scale conversion how to do\n",
    "        resize_before_lcd = tf.transpose(resize_before_lcd, perm=[0, 2, 3, 1])\n",
    "        lcd_images = self.simulate_LCD_display(resize_before_lcd)\n",
    "\n",
    "        temp_images = tf.transpose(org_images.clone(), perm=[0, 2, 3, 1])\n",
    "\n",
    "        rotate_images, rotate_lcd_images = self.random_rotation_3(temp_images, lcd_images)\n",
    "        # rotate_images = rotate_images.to(self.device)\n",
    "        # rotate_lcd_images = rotate_lcd_images.to(self.device).detach()\n",
    "\n",
    "        dim_images = adjust_contrast_and_brightness(rotate_images, beta=-60)\n",
    "\n",
    "        ## compute the rotate prediction\n",
    "        rotate_images = rotate_images.permute(0, 3, 1, 2)\n",
    "        rotate_images = rotate_images.float()\n",
    "        rotate_outputs = self.model(rotate_images)\n",
    "        _, rotate_pre = tf.math.reduce_max(rotate_outputs.data, 1)\n",
    "        rotate_percentage = tf.nn.softmax(rotate_outputs.clone(), dim=1) * 100 # dim to axis\n",
    "\n",
    "        ## compute the dim prediction\n",
    "        dim_images = tf.transpose(dim_images, perm=[0, 3, 1, 2])\n",
    "        dim_images = dim_images.float()\n",
    "        dim_outputs = self.model(dim_images)\n",
    "        _, dim_pre = tf.math.reduce_max(dim_outputs.data, 1)\n",
    "        dim_percentage = tf.nn.softmax(dim_outputs.clone().detach(), dim=1) * 100 # dim to axis\n",
    "\n",
    "\n",
    "        ## Deliver the MA\n",
    "        for step in range(self.steps):\n",
    "            print(\"Step: {}/{}\".format(step, self.steps))\n",
    "\n",
    "            cfa_img = self.simulate_CFA(rotate_lcd_images)\n",
    "            cfa_img_noise = cfa_img + self.noise\n",
    "\n",
    "            demosaic_img = self.demosaic_and_denoise(cfa_img_noise)\n",
    "            demosaic_img = demosaic_img.permute(0, 3, 1, 2)\n",
    "\n",
    "            ## Adjust the brightness\n",
    "            brighter_img = adjust_contrast_and_brightness(demosaic_img, beta=20)\n",
    "\n",
    "            at_images = tf.image.resize(brighter_img, [299, 299], method='bilinear')\n",
    "            at_images = at_images.float()\n",
    "            at_outputs = self.model(at_images)\n",
    "            _, at_pre = tf.math.reduce_max(at_outputs.data, 1)\n",
    "\n",
    "            at_percentage = tf.nn.softmax(at_outputs.clone(), dim=1) * 100 # dim to axis\n",
    "\n",
    "            if self.targeted:\n",
    "                adv_cost = self.adv_loss(at_outputs, (tf.cast(targeted_labels, tf.int64)))\n",
    "            else:\n",
    "                adv_cost = -1 * self.adv_loss(at_outputs, org_labels)\n",
    "\n",
    "            total_cost = adv_cost\n",
    "            print(\"Loss: \", total_cost, \"Adv loss: \", adv_cost)\n",
    "    \n",
    "            with tf.GradientTape() as tape:\n",
    "                total_cost = adv_cost\n",
    "            grads = tape.gradient(total_cost, self.noise)\n",
    "            self.noise = tf.stop_gradient(self.noise) - self.lr * tf.sign(grads)\n",
    "            self.noise = tf.clip_by_value(self.noise, clip_value_min=-self.noise_budget, clip_value_max=self.noise_budget)\n",
    "            self.noise = tf.Variable(self.noise, trainable=True)\n",
    "\n",
    "        at_images = tf.clip_by_value(at_images, min=0, max=255)\n",
    "\n",
    "        return at_images, rotate_images, dim_images, \\\n",
    "               at_pre, rotate_pre, dim_pre, \\\n",
    "               org_percentage, at_percentage, rotate_percentage, dim_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99aaa968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(tf.keras.layers.Layer):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalize, self).__init__()\n",
    "        self.mean = tf.constant(mean, dtype=tf.float32)\n",
    "        self.std = tf.constant(std, dtype=tf.float32)\n",
    "\n",
    "    def call(self, input):\n",
    "        input = input / 255.0\n",
    "        mean = tf.reshape(self.mean, shape=(1, 1, 1, 3))\n",
    "        std = tf.reshape(self.std, shape=(1, 1, 1, 3))\n",
    "        return (input - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3bfa578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 1 classes.\n",
      "Found 2 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "STEPS = 10\n",
    "\n",
    "class_idx = json.load(open(\"./data/imagenet_class_index.json\"))\n",
    "idx2label = [class_idx[str(k)][1] for k in range(len(class_idx))]\n",
    "class2label = [class_idx[str(k)][0] for k in range(len(class_idx))]\n",
    "\n",
    "transform = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.Resizing(299, 299),\n",
    "    tf.keras.layers.experimental.preprocessing.Normalization(\n",
    "        mean=[0.485, 0.456, 0.406], variance=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "norm_layer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "model = Sequential([\n",
    "    norm_layer,\n",
    "    InceptionV3(weights='imagenet')\n",
    "])\n",
    "\n",
    "model = model.compile()\n",
    "\n",
    "## Save the results of MA\n",
    "## SET TO TRUE IF WE WANT TO SAVE THE OUTPUT IMAGES\n",
    "Save_results = 'True'\n",
    "if Save_results == 'True':\n",
    "    savedir = './Results'\n",
    "    adv_dir = os.path.join(savedir, 'adv')\n",
    "    rotate_dir = os.path.join(savedir, 'rotate')\n",
    "    org_dir = os.path.join(savedir, 'org')\n",
    "    dim_dir = os.path.join(savedir, 'dim')\n",
    "    create_dir(adv_dir)\n",
    "    create_dir(rotate_dir)\n",
    "    create_dir(org_dir)\n",
    "    create_dir(dim_dir)\n",
    "\n",
    "## deffault settings\n",
    "noise_budget = 2\n",
    "batch_size = 10\n",
    "epoch = 1#int(1000 / batch_size)\n",
    "total = 0\n",
    "suc_cnt_at = 0\n",
    "suc_cnt_dim = 0\n",
    "suc_cnt_rotate = 0\n",
    "\n",
    "\n",
    "normal_data = image_folder_custom_label(root='./data/dataset/incepv3_data',\n",
    "                                        transform=transform,\n",
    "                                        idx2label=class2label)\n",
    "\n",
    "# normal_loader = tf.keras.utils.image_dataset_from_directory('./data/dataset/incepv3_data')\n",
    "# a, b = [img, lbl for (img, lbl) in normal_loader.take(-1)]\n",
    "# print(\"-\" * 70)\n",
    "# print(\"Noise_budget = \", noise_budget)\n",
    "# start = time.time()\n",
    "# for batch in range(epoch):\n",
    "\n",
    "#     print(\"-\" * 70)\n",
    "#     org_imgs, org_labels = [(img, lbl) for img, lbl in normal_loader.take(-1)]\n",
    "#     org_imgs = org_imgs * 255.0\n",
    "#     print('Epoch = ' + str(batch))\n",
    "\n",
    "#     targeted_labels = np.random.randint(0,999)\n",
    "#     targeted_labels = tf.convert_to_tensor(targeted_labels * np.ones((batch_size), dtype = np.int))\n",
    "#     targeted = True\n",
    "\n",
    "\n",
    "#     attack = Morie_attack(model,\n",
    "#                           noise_budget=noise_budget,\n",
    "#                           img_w=299,\n",
    "#                           img_h=299,\n",
    "#                           scale_factor=3,\n",
    "#                           targeted=targeted,\n",
    "#                           batch_size=batch_size,\n",
    "#                           steps=STEPS)\n",
    "    \n",
    "#     at_images, rotate_images, dim_images, \\\n",
    "#     at_labels, rotate_labels, dim_labels, \\\n",
    "#     org_percentges, at_percentages, rotate_percentages, dim_percentages = attack(org_imgs, org_labels, targeted_labels)\n",
    "#     org_labels = org_labels.to(device)\n",
    "#     targeted_labels = targeted_labels.to(device)\n",
    "#     rotate_labels = rotate_labels.to(device)\n",
    "\n",
    "#     ## compute the succes rate\n",
    "#     total += batch_size\n",
    "\n",
    "#     suc_cnt_rotate += (rotate_labels != org_labels).sum()\n",
    "#     suc_cnt_dim += (dim_labels != org_labels).sum()\n",
    "\n",
    "#     if targeted:\n",
    "#         suc_cnt_at += (at_labels == targeted_labels).sum()\n",
    "#     else:\n",
    "#         suc_cnt_at += (at_labels != org_labels).sum()\n",
    "\n",
    "\n",
    "#     Succ_cnt_rotate = (rotate_labels != org_labels).sum() / batch_size\n",
    "#     Succ_total_rotate = suc_cnt_rotate / total\n",
    "#     Succ_cnt_dim = (dim_labels != org_labels).sum() / batch_size\n",
    "#     Succ_total_dim = suc_cnt_dim / total\n",
    "#     if targeted:\n",
    "#         Succ_cnt_at = (at_labels == targeted_labels).sum() / batch_size\n",
    "#         Succ_total_at = suc_cnt_at / total\n",
    "#     else:\n",
    "#         Succ_cnt_at = (at_labels != org_labels).sum() / batch_size\n",
    "#         Succ_total_at = suc_cnt_at / total\n",
    "\n",
    "#     print(\"Current rotate Suc rate: \", Succ_cnt_rotate)\n",
    "#     print(\"Current dim Suc rate: \", Succ_cnt_dim)\n",
    "#     print(\"Current attack Suc rate: \", Succ_cnt_at)\n",
    "#     print(\"Total rotate Suc rate: \", Succ_total_rotate)\n",
    "#     print(\"Total dim Suc rate: \", Succ_total_dim)\n",
    "#     print(\"Total attack Suc rate: \", Succ_total_at)\n",
    "\n",
    "#     labels_np = org_labels.numpy()\n",
    "#     rotate_labels_np = rotate_labels.numpy()\n",
    "#     dim_labels_np = dim_labels.cpu().numpy()\n",
    "#     at_labels_np = at_labels.cpu().numpy()\n",
    "\n",
    "#     org_images_np = org_imgs.numpy()\n",
    "#     at_images_np = at_images.numpy()\n",
    "#     rotate_images_np = rotate_images.numpy()\n",
    "#     dim_images_np = dim_images.numpy()\n",
    "\n",
    "#     org_percentages_np = org_percentges.numpy()\n",
    "#     at_percentages_np = at_percentages.numpy()\n",
    "#     rotate_percentages_np = rotate_percentages.numpy()\n",
    "#     dim_percentages_np = dim_percentages.numpy()\n",
    "\n",
    "#     # save the pics\n",
    "#     for i in range(batch_size):\n",
    "#         print('allo')\n",
    "#         img_org = org_images_np[i]\n",
    "#         img_at = at_images_np[i]\n",
    "#         img_dim = dim_images_np[i]\n",
    "#         img_rotate = rotate_images_np[i]\n",
    "\n",
    "#         img_org = np.moveaxis(img_org, 0, 2)\n",
    "#         img_at = np.moveaxis(img_at, 0, 2)\n",
    "#         img_dim = np.moveaxis(img_dim, 0, 2)\n",
    "#         img_rotate = np.moveaxis(img_rotate, 0, 2)\n",
    "\n",
    "#         true_class = idx2label[labels_np[i]]\n",
    "#         at_class = idx2label[at_labels_np[i]]\n",
    "#         dim_class = idx2label[dim_labels_np[i]]\n",
    "#         rotate_class = idx2label[rotate_labels_np[i]]\n",
    "\n",
    "#         percentage_org = org_percentages_np[i][labels_np[i]]\n",
    "#         percentage_at = at_percentages_np[i][at_labels_np[i]]\n",
    "#         percentage_rotate = rotate_percentages_np[i][rotate_labels_np[i]]\n",
    "#         percentage_dim = dim_percentages_np[i][dim_labels_np[i]]\n",
    "\n",
    "#         if Save_results == 'True':\n",
    "#             # save org_images\n",
    "#             img_org_name = true_class + str(percentage_org) + \".JPEG\"\n",
    "#             img_org_path = os.path.join(org_dir, img_org_name)\n",
    "#             img_org_pil = Image.fromarray(img_org.astype(np.uint8))\n",
    "#             img_org_pil.save(img_org_path)\n",
    "\n",
    "#             # uncomment the following if you want to save the intermediate results\n",
    "#             # save rotated_images:\n",
    "#             img_rotate_name = true_class + str(percentage_org) + \"_\" + rotate_class + str(percentage_rotate) + \".JPEG\"\n",
    "#             img_rotate_path = os.path.join(rotate_dir, img_rotate_name)\n",
    "#             img_rotate_pil = Image.fromarray(img_rotate.astype(np.uint8))\n",
    "#             img_rotate_pil.save(img_rotate_path)\n",
    "\n",
    "#             # save dim_images:\n",
    "#             img_dim_name = true_class + str(percentage_org) + \"_\" + dim_class + str(percentage_dim) + \".JPEG\"\n",
    "#             img_dim_path = os.path.join(dim_dir, img_dim_name)\n",
    "#             img_dim_pil = Image.fromarray(img_dim.astype(np.uint8))\n",
    "#             img_dim_pil.save(img_dim_path)\n",
    "\n",
    "#             ##  save at_images:\n",
    "#             img_at_name = true_class + str(percentage_org) + \"_\" + at_class + str(percentage_at) + \".JPEG\"\n",
    "#             img_at_path = os.path.join(adv_dir, img_at_name)\n",
    "#             img_at_pil = Image.fromarray(img_at.astype(np.uint8))\n",
    "#             img_at_pil.save(img_at_path)\n",
    "\n",
    "\n",
    "\n",
    "#     del attack, at_images, rotate_images, dim_images, \\\n",
    "#         at_labels, rotate_labels, dim_labels, \\\n",
    "#         org_percentges, at_percentages, rotate_percentages, dim_percentages, \\\n",
    "#         org_labels, targeted_labels, \\\n",
    "#         org_imgs\n",
    "\n",
    "# print(\"Rotate Success rate: \", Succ_total_rotate)\n",
    "# print(\"dim Success rate: \", Succ_total_dim)\n",
    "# print(\"Attack Success rate: \", Succ_total_at)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "049f7bc2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 images belonging to 1 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x1d9075be648>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_data.image_data_generator.flow_from_directory('./data/dataset/incepv3_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e5b4e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.preprocessing.image.DirectoryIterator at 0x1d907916c88>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f36d0776",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DirectoryIterator' object has no attribute 'flow_from_directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21660\\3641389419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow_from_directory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/dataset/incepv3_data'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DirectoryIterator' object has no attribute 'flow_from_directory'"
     ]
    }
   ],
   "source": [
    "images, labels = next(normal_data.flow_from_directory('./data/dataset/incepv3_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f470b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageDataGenerator(validation_split=0.0)\n",
    "old_data = data_gen.flow_from_directory('./data/dataset/incepv3_data', target_size=(224, 224), class_mode='sparse')\n",
    "old_classes = list(old_data.class_indices.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c73b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Moire_Attack_env",
   "language": "python",
   "name": "moire_attack_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
